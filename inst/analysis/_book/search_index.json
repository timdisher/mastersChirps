[["pca-analysis.html", "2 PCA Analysis 2.1 Example", " 2 PCA Analysis 2.1 Example A brief set of example code was provided by Ryan based on this tutorial. The analysis is based on the mtcars dataset and provides a very bare bones showcase of the prcomp function and ggbiplot. First we create the PCA object using prcomp limited to continuous variables only. This was done because the other variables in mtcars are categorical/ordinal which do not work well with PCA without further processing. mtcars.pca &lt;- prcomp(mtcars[,c(1:7,10,11)], center = TRUE, scale. = TRUE) summary(mtcars.pca) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 ## Standard deviation 2.3782 1.4429 0.71008 0.51481 0.42797 0.35184 0.32413 0.2419 0.14896 ## Proportion of Variance 0.6284 0.2313 0.05602 0.02945 0.02035 0.01375 0.01167 0.0065 0.00247 ## Cumulative Proportion 0.6284 0.8598 0.91581 0.94525 0.96560 0.97936 0.99103 0.9975 1.00000 We can them make a simple plot to show how different variables are related. The post goes into some extra details around drawing ellipses etc but we won’t get into that until the analysis phase if there is something interesting. One of the concepts that they showed that was pretty interesting though was how this type of plot can help you to identify variables that might be important for classification problems. For example all of the american cars cluster tightly in one section of the plot, suggesting they would be important in an “American vs not American” classification. ggbiplot(mtcars.pca,ellipse=TRUE,choices=c(3,4), labels=rownames(mtcars)) "]]
